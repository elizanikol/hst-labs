# HST-MPI

## Бизнес логика:
Сформировать M результирующих векторов как квадрат максимального значения по каждой строке M исходных квадратных матриц.

## Вариант исполнения:
Реализация через MPI.

## Особенности исполнения:
Считывание данных происходит из бинарного файла;
Данные генерируются утилитой, принимающей в качестве параметров размер данных для обработки в мегабайтах и имя файла;
Программа выполняет бизнес-логику и записывает результат в выходной файл;
В конце файла с результатами сохраняется информация о времени выполнения вычислений и размере обработанных данных.

## Описание алгоритма выполнения бизнес-логики:
1. Для генерации исходных данных используется функция ```GenerateMatricesAndSaveThemToFile()```. 
С помощью ```std::default_random_engine``` определяется количество строк очередной квадратной матрицы, которая затем заполняется произвольными значениями типа double в диапазоне [0, 10] и записывается в бинарный файл ```InputDataFile```. 
Генерация матриц заканчивается после того, как размер файла с данными превысит размер, введенный пользователем.

2. После этого запускается программа-клиент, считывающая сгенерированные матрицы из ```InputDataFile```. В функции ```CalculateMaxElementsSquare()``` над исходными данными выполняются вычисления: для каждой строки i-ой матрицы с помощью функции MaxElement() вычисляется максимальное значение, которое возводится в квадрат и записывается в результирующий i-ый вектор.
3. Определяется время выполнения вычислений, которое записывается в бинарный файл ```ResultsFile``` вместе с результатами вычислений.

## Описание логики распараллеливания:
В главном потоке, имеющем rank = MasterID (0), выполняется чтение исходных данных из файла и распределение их между остальными процессами. Количество матриц, передаваемое процессу, вычисляется с помощью функции:
```
size_t MatricesNumberForProcess(size_t processId, size_t matricesSize, size_t processesTotalNumber)
{
    const auto avgMatricesNumberPerProcess = matricesSize / processesTotalNumber;
    const auto remainder = matricesSize % processesTotalNumber;
    return avgMatricesNumberPerProcess + (processId < remainder);
}
```
При этом каждому процессу сначала предполагается назначить одинаковое количество матриц для обработки, а оставшиеся после этого матрицы по очереди добавляются по одной к каждому из первых процессов в списке.

Пересылка данных между мастером и остальными процессами осуществляется с помощью функций MPI_Send и MPI_Recv. При этом в главном процессе в переменных ```matricesNumberForEachProcess``` и ```matricesSizesForEachProcess``` сохраняется количество и размер пересылаемых матриц каждому процессу для повторного использования этих значений при сборе данных после выполнения вычислений.

Для определения времени вычислений используется функция MPI_Wtime(), вызываемая из основного потока перед и после выполнения вычислений всеми процессами.

## График зависимости времени вычислений от размера исходных данных:
Результаты приведены для 4 потоков.
![Снимок экрана 2022-11-12 133221](https://user-images.githubusercontent.com/55412039/201471189-c6d768a6-0b6e-4e57-90b0-8708e2307d72.png)

